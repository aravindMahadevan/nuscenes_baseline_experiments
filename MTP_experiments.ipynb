{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.eval.prediction.config import load_prediction_config\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.prediction.models.physics import ConstantVelocityHeading, PhysicsOracle\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.models.mtp import MTP, MTPLoss\n",
    "from nuscenes.prediction.models.covernet import CoverNet, ConstantLatticeLoss\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.eval.prediction.data_classes import Prediction\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/armahade/ECE285/AV/final_proj/MTP_experiments/maps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), 'maps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.eval.prediction.config import load_prediction_config\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.prediction import PredictHelper\n",
    "\n",
    "from nuscenes.prediction.models.physics import ConstantVelocityHeading, PhysicsOracle\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "from PIL import Image\n",
    "\n",
    "class NuscenesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, NuscenesBaseDir, \n",
    "                 data_set_version = 'v1.0-trainval', \n",
    "                 maps_dir = 'maps',\n",
    "                 save_maps_dataset = False, \n",
    "                 split = 'train_val',\n",
    "                 config_name = 'predict_2020_icra.json',\n",
    "                 history=1, \n",
    "                 in_agent_frame=True):\n",
    "        \n",
    "        # path to main dataset\n",
    "        self.baseDir = NuscenesBaseDir\n",
    "        self.nusc = NuScenes(version=data_set_version, dataroot=self.baseDir, verbose=True)\n",
    "        self.helper = PredictHelper(self.nusc)\n",
    "        \n",
    "        #initialize maps directory where everything will be saved \n",
    "        self.maps_dir = os.path.join(os.getcwd(), 'maps')\n",
    "\n",
    "        #initialize the data set \n",
    "        self.data_set = get_prediction_challenge_split(split,dataroot=self.baseDir)\n",
    "\n",
    "        #initialize rasterizers for the \n",
    "        self.static_layer_rasterizer = StaticLayerRasterizer(self.helper)\n",
    "        self.agent_rasterizer = AgentBoxesWithFadedHistory(self.helper, seconds_of_history=history)\n",
    "        self.mtp_input_representation = InputRepresentation(self.static_layer_rasterizer, self.agent_rasterizer, Rasterizer())\n",
    "\n",
    "        self.in_agent_frame = in_agent_frame\n",
    "\n",
    "        self.config = load_prediction_config(self.helper, config_name)\n",
    "\n",
    "        self.valid_data_points = []\n",
    "        \n",
    "        self.save_maps_dataset = save_maps_dataset\n",
    "        \n",
    "        if self.save_maps_dataset: \n",
    "            self.save_maps()\n",
    "            \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            \n",
    "    def save_maps(self):\n",
    "        '''\n",
    "        Input: None \n",
    "        Output: None\n",
    "        \n",
    "        This method finds all the valid data points in the data set. We define a valid data point \n",
    "        where the velocity, acceleartion, and heading specified by token not NaN. \n",
    "        '''\n",
    "        print(\"starting filtering and creation of data set\")\n",
    "        for i, token in enumerate(self.data_set): \n",
    "            instance_token_img, sample_token_img = self.data_set[i].split('_')\n",
    "            \n",
    "            file_path = os.path.join(self.maps_dir, \"maps_{0}.jpg\".format(i))\n",
    "            \n",
    "            instance_token_img, sample_token_img = self.data_set[i].split('_')\n",
    "            img = self.mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)\n",
    "            im = Image.fromarray(img)\n",
    "            im.save(file_path)\n",
    "        \n",
    "            print(\"{0}/{1} image saved\".format(i, len(self.data_set)))\n",
    "            \n",
    "        print(\"done filtering data set \")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "    \n",
    "    #return the image tensor, agent state vector, and the ground truth\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        instance_token_img, sample_token_img = self.data_set[index].split('_')\n",
    "        \n",
    "        velocity = self.helper.get_velocity_for_agent(instance_token_img, sample_token_img)\n",
    "        acceleration = self.helper.get_acceleration_for_agent(instance_token_img, sample_token_img)\n",
    "        heading = self.helper.get_heading_change_rate_for_agent(instance_token_img, sample_token_img)        \n",
    "\n",
    "        #using a padding token of -1\n",
    "        if np.isnan(velocity) or np.isnan(acceleration) or np.isnan(heading):\n",
    "            velocity =  acceleration = heading = -1 \n",
    "\n",
    "        #construct agent state vector \n",
    "        agent_state_vec = torch.Tensor([velocity, acceleration, heading])\n",
    "        \n",
    "        #change image from (3, N, N), will have data loader take care \n",
    "        #get image and construct tensor \n",
    "        file_path = os.path.join(self.maps_dir, \"maps_{0}.jpg\".format(index))\n",
    "        \n",
    "        im = Image.open(file_path)\n",
    "        img = np.array(im)\n",
    "        image_tensor = torch.Tensor(img).permute(2, 0, 1)\n",
    "        \n",
    "        #get ground truth \n",
    "        ground_truth = self.helper.get_future_for_agent(instance_token_img, \n",
    "                                                        sample_token_img,\n",
    "                                                        self.config.seconds, \n",
    "                                                        in_agent_frame=self.in_agent_frame)\n",
    "        \n",
    "        ground_truth = torch.Tensor(ground_truth).unsqueeze(0)\n",
    "        return image_tensor, agent_state_vec, ground_truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 40.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 13.2 seconds.\n",
      "======\n",
      "static_layers.py - Loading Map: singapore-queenstown\n",
      "static_layers.py - Loading Map: singapore-hollandvillage\n",
      "static_layers.py - Loading Map: boston-seaport\n",
      "static_layers.py - Loading Map: singapore-onenorth\n",
      "static_layers.py - Loading Map: singapore-queenstown\n",
      "static_layers.py - Loading Map: singapore-hollandvillage\n",
      "static_layers.py - Loading Map: boston-seaport\n",
      "static_layers.py - Loading Map: singapore-onenorth\n"
     ]
    }
   ],
   "source": [
    "dataset = NuscenesDataset( '../full_data/sets/nuscenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtp = MTP(backbone, num_modes=M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EPSILON_8_SET = \"./nuscenes-prediction-challenge-trajectory-sets/epsilon_8.pkl\"\n",
    "trajectories = pickle.load(open(PATH_TO_EPSILON_8_SET, 'rb'))\n",
    "\n",
    "#Saved them as a list of lists\n",
    "trajectories = torch.Tensor(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:53\n",
      "starting 100 epochs\n",
      "total batches:1070\n",
      "2/1070 loss is: 9.904657363891602\n",
      "3/1070 loss is: 21.824880599975586\n",
      "4/1070 loss is: 13.754898071289062\n",
      "5/1070 loss is: 16.572734832763672\n",
      "6/1070 loss is: 9.820317268371582\n",
      "7/1070 loss is: 14.750350952148438\n",
      "8/1070 loss is: 15.147744178771973\n",
      "9/1070 loss is: 14.552214622497559\n",
      "10/1070 loss is: 13.717880249023438\n",
      "11/1070 loss is: 7.0016374588012695\n",
      "12/1070 loss is: 11.460454940795898\n",
      "13/1070 loss is: 18.642410278320312\n",
      "14/1070 loss is: 13.952999114990234\n",
      "15/1070 loss is: 15.950462341308594\n",
      "16/1070 loss is: 13.097911834716797\n",
      "17/1070 loss is: 15.3785400390625\n",
      "18/1070 loss is: 11.18424129486084\n",
      "19/1070 loss is: 12.549555778503418\n",
      "20/1070 loss is: 10.487478256225586\n",
      "21/1070 loss is: 11.82686996459961\n",
      "22/1070 loss is: 10.422770500183105\n",
      "23/1070 loss is: 15.56697940826416\n",
      "24/1070 loss is: 10.63955020904541\n",
      "25/1070 loss is: 10.564567565917969\n",
      "26/1070 loss is: 9.383420944213867\n",
      "27/1070 loss is: 11.852909088134766\n",
      "28/1070 loss is: 11.074748992919922\n",
      "29/1070 loss is: 12.770156860351562\n",
      "30/1070 loss is: 8.365553855895996\n",
      "31/1070 loss is: 8.388867378234863\n",
      "32/1070 loss is: 9.689459800720215\n",
      "33/1070 loss is: 8.732321739196777\n",
      "34/1070 loss is: 7.84329891204834\n",
      "35/1070 loss is: 8.925729751586914\n",
      "36/1070 loss is: 9.978691101074219\n",
      "37/1070 loss is: 9.563386917114258\n",
      "38/1070 loss is: 7.921399116516113\n",
      "39/1070 loss is: 7.369203090667725\n",
      "40/1070 loss is: 11.735156059265137\n",
      "41/1070 loss is: 9.159120559692383\n",
      "42/1070 loss is: 6.428380012512207\n",
      "43/1070 loss is: 10.0095796585083\n",
      "44/1070 loss is: 5.326717376708984\n",
      "45/1070 loss is: 6.404245376586914\n",
      "46/1070 loss is: 7.799409866333008\n",
      "47/1070 loss is: 9.071263313293457\n",
      "48/1070 loss is: 6.176918029785156\n",
      "49/1070 loss is: 11.338932991027832\n",
      "50/1070 loss is: 5.584018707275391\n",
      "51/1070 loss is: 6.982766151428223\n",
      "52/1070 loss is: 5.042590618133545\n",
      "53/1070 loss is: 7.1523284912109375\n",
      "54/1070 loss is: 4.160382270812988\n",
      "55/1070 loss is: 8.984420776367188\n",
      "56/1070 loss is: 4.940701484680176\n",
      "57/1070 loss is: 6.360724925994873\n",
      "58/1070 loss is: 4.986005783081055\n",
      "59/1070 loss is: 4.850813865661621\n",
      "60/1070 loss is: 5.692127704620361\n",
      "61/1070 loss is: 7.673637866973877\n",
      "62/1070 loss is: 5.190916538238525\n",
      "63/1070 loss is: 6.263662815093994\n",
      "64/1070 loss is: 3.7890844345092773\n",
      "65/1070 loss is: 7.439177513122559\n",
      "66/1070 loss is: 7.622052192687988\n",
      "67/1070 loss is: 8.222818374633789\n",
      "68/1070 loss is: 6.777743339538574\n",
      "69/1070 loss is: 4.257807731628418\n",
      "70/1070 loss is: 8.91193675994873\n",
      "71/1070 loss is: 3.452070713043213\n",
      "72/1070 loss is: 8.643186569213867\n",
      "73/1070 loss is: 5.711944580078125\n",
      "74/1070 loss is: 7.55391788482666\n",
      "75/1070 loss is: 3.704359769821167\n",
      "76/1070 loss is: 6.248366355895996\n",
      "77/1070 loss is: 5.578690528869629\n",
      "78/1070 loss is: 6.208409786224365\n",
      "79/1070 loss is: 6.0531907081604\n",
      "80/1070 loss is: 5.003544330596924\n",
      "81/1070 loss is: 7.158280372619629\n",
      "82/1070 loss is: 5.425154685974121\n",
      "83/1070 loss is: 3.244654655456543\n",
      "84/1070 loss is: 6.698385238647461\n",
      "85/1070 loss is: 4.233585357666016\n",
      "86/1070 loss is: 5.549999237060547\n",
      "87/1070 loss is: 6.086268901824951\n",
      "88/1070 loss is: 4.478116989135742\n",
      "89/1070 loss is: 6.146430492401123\n",
      "90/1070 loss is: 4.0520429611206055\n",
      "91/1070 loss is: 6.069421768188477\n",
      "92/1070 loss is: 7.967634201049805\n",
      "93/1070 loss is: 6.182980537414551\n",
      "94/1070 loss is: 5.9634552001953125\n",
      "95/1070 loss is: 4.077159881591797\n",
      "96/1070 loss is: 6.488008499145508\n",
      "97/1070 loss is: 7.793774604797363\n",
      "98/1070 loss is: 4.017761707305908\n",
      "99/1070 loss is: 4.605066776275635\n",
      "100/1070 loss is: 4.89841890335083\n",
      "101/1070 loss is: 6.683588981628418\n",
      "102/1070 loss is: 6.581750869750977\n",
      "103/1070 loss is: 9.058274269104004\n",
      "104/1070 loss is: 4.559175491333008\n",
      "105/1070 loss is: 5.422781944274902\n",
      "106/1070 loss is: 4.602209091186523\n",
      "107/1070 loss is: 4.652290344238281\n",
      "108/1070 loss is: 3.372605800628662\n",
      "109/1070 loss is: 4.784306526184082\n",
      "110/1070 loss is: 9.255255699157715\n",
      "111/1070 loss is: 5.93106746673584\n",
      "112/1070 loss is: 5.531166076660156\n",
      "113/1070 loss is: 5.124659538269043\n",
      "114/1070 loss is: 8.527288436889648\n",
      "115/1070 loss is: 7.422621250152588\n",
      "116/1070 loss is: 5.524845123291016\n",
      "117/1070 loss is: 4.0301055908203125\n",
      "118/1070 loss is: 4.392330646514893\n",
      "119/1070 loss is: 7.249205112457275\n",
      "120/1070 loss is: 5.019242286682129\n",
      "121/1070 loss is: 4.551821708679199\n",
      "122/1070 loss is: 7.168484210968018\n",
      "123/1070 loss is: 5.055782318115234\n",
      "124/1070 loss is: 2.5250844955444336\n",
      "125/1070 loss is: 7.4852495193481445\n",
      "126/1070 loss is: 2.784214496612549\n",
      "127/1070 loss is: 3.3026275634765625\n",
      "128/1070 loss is: 3.0701496601104736\n",
      "129/1070 loss is: 3.6203126907348633\n",
      "130/1070 loss is: 4.915820121765137\n",
      "131/1070 loss is: 5.173153400421143\n",
      "132/1070 loss is: 3.839820146560669\n",
      "133/1070 loss is: 4.792842864990234\n",
      "134/1070 loss is: 6.020501136779785\n",
      "135/1070 loss is: 6.074649810791016\n",
      "136/1070 loss is: 4.474778175354004\n",
      "137/1070 loss is: 2.9269747734069824\n",
      "138/1070 loss is: 6.418825626373291\n",
      "139/1070 loss is: 3.7505903244018555\n",
      "140/1070 loss is: 6.293004512786865\n",
      "141/1070 loss is: 7.411700248718262\n",
      "142/1070 loss is: 4.994463920593262\n",
      "143/1070 loss is: 4.273754119873047\n",
      "144/1070 loss is: 5.147083282470703\n",
      "145/1070 loss is: 4.475225448608398\n",
      "146/1070 loss is: 3.072547197341919\n",
      "147/1070 loss is: 3.2630081176757812\n",
      "148/1070 loss is: 6.102934837341309\n",
      "149/1070 loss is: 6.578616142272949\n",
      "150/1070 loss is: 5.162902355194092\n",
      "151/1070 loss is: 3.0660552978515625\n",
      "152/1070 loss is: 5.138197898864746\n",
      "153/1070 loss is: 3.590601921081543\n",
      "154/1070 loss is: 5.975710868835449\n",
      "155/1070 loss is: 7.2596025466918945\n",
      "156/1070 loss is: 6.109960079193115\n",
      "157/1070 loss is: 4.302891731262207\n",
      "158/1070 loss is: 5.454942226409912\n",
      "159/1070 loss is: 4.924506187438965\n",
      "160/1070 loss is: 3.45202898979187\n",
      "161/1070 loss is: 3.143479585647583\n",
      "162/1070 loss is: 4.815247058868408\n",
      "163/1070 loss is: 6.114532947540283\n",
      "164/1070 loss is: 3.716757297515869\n",
      "165/1070 loss is: 3.752737522125244\n",
      "166/1070 loss is: 3.719008445739746\n",
      "167/1070 loss is: 8.288589477539062\n",
      "168/1070 loss is: 4.662142276763916\n",
      "169/1070 loss is: 4.429518222808838\n",
      "170/1070 loss is: 7.547034740447998\n",
      "171/1070 loss is: 6.9679107666015625\n",
      "172/1070 loss is: 6.4624834060668945\n",
      "173/1070 loss is: 5.291600704193115\n",
      "174/1070 loss is: 5.653244495391846\n",
      "175/1070 loss is: 3.8400330543518066\n",
      "176/1070 loss is: 4.31386661529541\n",
      "177/1070 loss is: 5.8958587646484375\n",
      "178/1070 loss is: 5.369122505187988\n",
      "179/1070 loss is: 6.262168884277344\n",
      "180/1070 loss is: 6.644443511962891\n",
      "181/1070 loss is: 5.908461570739746\n",
      "182/1070 loss is: 4.330073833465576\n",
      "183/1070 loss is: 4.345652103424072\n",
      "184/1070 loss is: 3.1416306495666504\n",
      "185/1070 loss is: 5.632658958435059\n",
      "186/1070 loss is: 5.505441665649414\n",
      "187/1070 loss is: 5.641070365905762\n",
      "188/1070 loss is: 7.984444618225098\n",
      "189/1070 loss is: 4.956085681915283\n",
      "190/1070 loss is: 6.182890892028809\n",
      "191/1070 loss is: 4.679434299468994\n",
      "192/1070 loss is: 5.3028669357299805\n",
      "193/1070 loss is: 4.670413970947266\n",
      "194/1070 loss is: 5.596199989318848\n",
      "195/1070 loss is: 3.1451756954193115\n",
      "196/1070 loss is: 4.853440761566162\n",
      "197/1070 loss is: 5.409064292907715\n",
      "198/1070 loss is: 4.773265838623047\n",
      "199/1070 loss is: 4.295494079589844\n",
      "200/1070 loss is: 3.0314619541168213\n",
      "201/1070 loss is: 4.351180076599121\n",
      "202/1070 loss is: 3.709613800048828\n",
      "203/1070 loss is: 2.4442076683044434\n",
      "204/1070 loss is: 4.341497898101807\n",
      "205/1070 loss is: 7.198607444763184\n",
      "206/1070 loss is: 6.4151763916015625\n",
      "207/1070 loss is: 3.9924323558807373\n",
      "208/1070 loss is: 6.731235027313232\n",
      "209/1070 loss is: 3.703824520111084\n",
      "210/1070 loss is: 4.511722564697266\n",
      "211/1070 loss is: 4.1954874992370605\n",
      "212/1070 loss is: 3.5791428089141846\n",
      "213/1070 loss is: 7.462750434875488\n",
      "214/1070 loss is: 4.440029621124268\n",
      "215/1070 loss is: 5.641034126281738\n",
      "216/1070 loss is: 5.013986110687256\n",
      "217/1070 loss is: 5.378318786621094\n",
      "218/1070 loss is: 3.314272880554199\n",
      "219/1070 loss is: 3.7121520042419434\n",
      "220/1070 loss is: 4.547706604003906\n",
      "221/1070 loss is: 4.117712020874023\n",
      "222/1070 loss is: 2.933206081390381\n",
      "223/1070 loss is: 6.594184875488281\n",
      "224/1070 loss is: 4.677697658538818\n",
      "225/1070 loss is: 5.5617804527282715\n",
      "226/1070 loss is: 4.733672142028809\n",
      "227/1070 loss is: 7.591615200042725\n",
      "228/1070 loss is: 5.684692859649658\n",
      "229/1070 loss is: 3.498711109161377\n",
      "230/1070 loss is: 4.349180221557617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/1070 loss is: 3.539118766784668\n",
      "232/1070 loss is: 4.042892932891846\n",
      "233/1070 loss is: 6.357214450836182\n",
      "234/1070 loss is: 6.524319171905518\n",
      "235/1070 loss is: 4.8041887283325195\n",
      "236/1070 loss is: 8.090332984924316\n",
      "237/1070 loss is: 4.553734302520752\n",
      "238/1070 loss is: 4.92221736907959\n",
      "239/1070 loss is: 5.076413631439209\n",
      "240/1070 loss is: 4.489249229431152\n",
      "241/1070 loss is: 4.9366254806518555\n",
      "242/1070 loss is: 4.624505519866943\n",
      "243/1070 loss is: 5.161996841430664\n",
      "244/1070 loss is: 4.948222637176514\n",
      "245/1070 loss is: 4.571588516235352\n",
      "246/1070 loss is: 3.6597845554351807\n",
      "247/1070 loss is: 6.010866165161133\n",
      "248/1070 loss is: 4.504298210144043\n",
      "249/1070 loss is: 4.741362571716309\n",
      "250/1070 loss is: 5.587397575378418\n",
      "251/1070 loss is: 10.113014221191406\n",
      "252/1070 loss is: 6.2922043800354\n",
      "253/1070 loss is: 6.703742980957031\n",
      "254/1070 loss is: 5.449493408203125\n",
      "255/1070 loss is: 5.380086898803711\n",
      "256/1070 loss is: 4.604427337646484\n",
      "257/1070 loss is: 6.478140354156494\n",
      "258/1070 loss is: 7.173451900482178\n",
      "259/1070 loss is: 5.9451494216918945\n",
      "260/1070 loss is: 4.120818138122559\n",
      "261/1070 loss is: 5.060020923614502\n",
      "262/1070 loss is: 6.288027286529541\n",
      "263/1070 loss is: 5.636187553405762\n",
      "264/1070 loss is: 6.036284923553467\n",
      "265/1070 loss is: 3.554410696029663\n",
      "266/1070 loss is: 8.032464981079102\n",
      "267/1070 loss is: 5.021443843841553\n",
      "268/1070 loss is: 4.516066074371338\n",
      "269/1070 loss is: 4.372760772705078\n",
      "270/1070 loss is: 4.1828742027282715\n",
      "271/1070 loss is: 3.996758222579956\n",
      "272/1070 loss is: 4.327747344970703\n",
      "273/1070 loss is: 4.9906206130981445\n",
      "274/1070 loss is: 4.695265769958496\n",
      "275/1070 loss is: 6.439535140991211\n",
      "276/1070 loss is: 3.989677906036377\n",
      "277/1070 loss is: 6.455343246459961\n",
      "278/1070 loss is: 6.285022735595703\n",
      "279/1070 loss is: 5.897404670715332\n",
      "280/1070 loss is: 6.013123035430908\n",
      "281/1070 loss is: 3.2288661003112793\n",
      "282/1070 loss is: 3.1475677490234375\n",
      "283/1070 loss is: 4.647683143615723\n",
      "284/1070 loss is: 6.20756721496582\n",
      "285/1070 loss is: 3.00533127784729\n",
      "286/1070 loss is: 6.615961074829102\n",
      "287/1070 loss is: 6.3984694480896\n",
      "288/1070 loss is: 3.8712716102600098\n",
      "289/1070 loss is: 3.291597843170166\n",
      "290/1070 loss is: 4.1582512855529785\n",
      "291/1070 loss is: 4.293402671813965\n",
      "292/1070 loss is: 7.499566078186035\n",
      "293/1070 loss is: 2.9514646530151367\n",
      "294/1070 loss is: 4.806523323059082\n",
      "295/1070 loss is: 2.850111484527588\n",
      "296/1070 loss is: 5.604551315307617\n",
      "297/1070 loss is: 4.082160949707031\n",
      "298/1070 loss is: 3.9328384399414062\n",
      "299/1070 loss is: 6.286985397338867\n",
      "300/1070 loss is: 4.239751815795898\n",
      "301/1070 loss is: 5.190173149108887\n",
      "302/1070 loss is: 4.993946075439453\n",
      "303/1070 loss is: 3.984386682510376\n",
      "304/1070 loss is: 5.299696922302246\n",
      "305/1070 loss is: 2.921410083770752\n",
      "306/1070 loss is: 3.2913777828216553\n",
      "307/1070 loss is: 3.332841396331787\n",
      "308/1070 loss is: 5.9860711097717285\n",
      "309/1070 loss is: 5.4780192375183105\n",
      "310/1070 loss is: 6.722306251525879\n",
      "311/1070 loss is: 5.7440266609191895\n",
      "312/1070 loss is: 4.218175888061523\n",
      "313/1070 loss is: 4.230526924133301\n",
      "314/1070 loss is: 5.721192359924316\n",
      "315/1070 loss is: 4.384819030761719\n",
      "316/1070 loss is: 7.711277008056641\n",
      "317/1070 loss is: 5.60154390335083\n",
      "318/1070 loss is: 7.090116024017334\n",
      "319/1070 loss is: 6.1177520751953125\n",
      "320/1070 loss is: 4.511269569396973\n",
      "321/1070 loss is: 6.2377519607543945\n",
      "322/1070 loss is: 4.568896770477295\n",
      "323/1070 loss is: 3.392669677734375\n",
      "324/1070 loss is: 3.87217378616333\n",
      "325/1070 loss is: 4.315335273742676\n",
      "326/1070 loss is: 4.592160224914551\n",
      "327/1070 loss is: 5.626325607299805\n",
      "328/1070 loss is: 3.3069748878479004\n",
      "329/1070 loss is: 6.043560028076172\n",
      "330/1070 loss is: 3.9048714637756348\n",
      "331/1070 loss is: 3.827176570892334\n",
      "332/1070 loss is: 2.3756916522979736\n",
      "333/1070 loss is: 4.872130393981934\n",
      "334/1070 loss is: 4.897421836853027\n",
      "335/1070 loss is: 4.0733747482299805\n",
      "336/1070 loss is: 4.114596366882324\n",
      "337/1070 loss is: 5.00153112411499\n",
      "338/1070 loss is: 4.598850727081299\n",
      "339/1070 loss is: 5.287191390991211\n",
      "340/1070 loss is: 5.340280532836914\n",
      "341/1070 loss is: 4.617240905761719\n",
      "342/1070 loss is: 3.7469348907470703\n",
      "343/1070 loss is: 3.162142753601074\n",
      "344/1070 loss is: 3.58406925201416\n",
      "345/1070 loss is: 2.390888214111328\n",
      "346/1070 loss is: 3.837407112121582\n",
      "347/1070 loss is: 5.372828960418701\n",
      "348/1070 loss is: 3.2617413997650146\n",
      "349/1070 loss is: 6.402144432067871\n",
      "350/1070 loss is: 4.485344409942627\n",
      "351/1070 loss is: 3.910795211791992\n",
      "352/1070 loss is: 5.207207679748535\n",
      "353/1070 loss is: 4.661065578460693\n",
      "354/1070 loss is: 6.229921817779541\n",
      "355/1070 loss is: 6.385364532470703\n",
      "356/1070 loss is: 6.084988117218018\n",
      "357/1070 loss is: 5.085456848144531\n",
      "358/1070 loss is: 3.2339863777160645\n",
      "359/1070 loss is: 5.833607196807861\n",
      "360/1070 loss is: 3.9211771488189697\n",
      "361/1070 loss is: 4.675864219665527\n",
      "362/1070 loss is: 4.264638900756836\n",
      "363/1070 loss is: 4.030576229095459\n",
      "364/1070 loss is: 4.5059919357299805\n",
      "365/1070 loss is: 5.412701606750488\n",
      "366/1070 loss is: 3.29191517829895\n",
      "367/1070 loss is: 3.9169750213623047\n",
      "368/1070 loss is: 4.83710241317749\n",
      "369/1070 loss is: 3.688760280609131\n",
      "370/1070 loss is: 6.193235397338867\n",
      "371/1070 loss is: 2.6615188121795654\n",
      "372/1070 loss is: 4.922965049743652\n",
      "373/1070 loss is: 5.273874282836914\n",
      "374/1070 loss is: 7.022267818450928\n",
      "375/1070 loss is: 5.448769569396973\n",
      "376/1070 loss is: 6.351874828338623\n",
      "377/1070 loss is: 2.538299560546875\n",
      "378/1070 loss is: 3.611636161804199\n",
      "379/1070 loss is: 2.561819314956665\n",
      "380/1070 loss is: 5.809415817260742\n",
      "381/1070 loss is: 6.040796279907227\n",
      "382/1070 loss is: 3.732816219329834\n",
      "383/1070 loss is: 3.532243251800537\n",
      "384/1070 loss is: 5.458648681640625\n",
      "385/1070 loss is: 6.735459327697754\n",
      "386/1070 loss is: 3.8813533782958984\n",
      "387/1070 loss is: 3.5171010494232178\n",
      "388/1070 loss is: 7.0958662033081055\n",
      "389/1070 loss is: 4.815277099609375\n",
      "390/1070 loss is: 4.079648017883301\n",
      "391/1070 loss is: 4.937337875366211\n",
      "392/1070 loss is: 6.495316505432129\n",
      "393/1070 loss is: 5.838301181793213\n",
      "394/1070 loss is: 4.835965156555176\n",
      "395/1070 loss is: 4.164843559265137\n",
      "396/1070 loss is: 4.377496242523193\n",
      "397/1070 loss is: 4.824041366577148\n",
      "398/1070 loss is: 5.881146430969238\n",
      "399/1070 loss is: 4.560909271240234\n",
      "400/1070 loss is: 5.502215385437012\n",
      "401/1070 loss is: 4.804876804351807\n",
      "402/1070 loss is: 5.239047050476074\n",
      "403/1070 loss is: 5.357597351074219\n",
      "404/1070 loss is: 5.848474502563477\n",
      "405/1070 loss is: 5.118013381958008\n",
      "406/1070 loss is: 4.5044379234313965\n",
      "407/1070 loss is: 3.3069019317626953\n",
      "408/1070 loss is: 5.003844738006592\n",
      "409/1070 loss is: 3.820998430252075\n",
      "410/1070 loss is: 3.7079811096191406\n",
      "411/1070 loss is: 4.928897857666016\n",
      "412/1070 loss is: 3.3807897567749023\n",
      "413/1070 loss is: 3.720730781555176\n",
      "414/1070 loss is: 8.260501861572266\n",
      "415/1070 loss is: 6.219696998596191\n",
      "416/1070 loss is: 3.801093339920044\n",
      "417/1070 loss is: 3.5550971031188965\n",
      "418/1070 loss is: 5.287550926208496\n",
      "419/1070 loss is: 5.157098293304443\n",
      "420/1070 loss is: 5.359932899475098\n",
      "421/1070 loss is: 5.505227565765381\n",
      "422/1070 loss is: 4.031396865844727\n",
      "423/1070 loss is: 4.726718902587891\n",
      "424/1070 loss is: 4.389509677886963\n",
      "425/1070 loss is: 4.574112415313721\n",
      "426/1070 loss is: 4.82404899597168\n",
      "427/1070 loss is: 5.3788042068481445\n",
      "428/1070 loss is: 6.140324115753174\n",
      "429/1070 loss is: 3.1630282402038574\n",
      "430/1070 loss is: 5.146265983581543\n",
      "431/1070 loss is: 2.7832117080688477\n",
      "432/1070 loss is: 4.521573543548584\n",
      "433/1070 loss is: 5.823771953582764\n",
      "434/1070 loss is: 4.474390506744385\n",
      "435/1070 loss is: 2.7415599822998047\n",
      "436/1070 loss is: 2.9784414768218994\n",
      "437/1070 loss is: 5.907386779785156\n",
      "438/1070 loss is: 5.358076572418213\n",
      "439/1070 loss is: 6.335654258728027\n",
      "440/1070 loss is: 4.280296325683594\n",
      "441/1070 loss is: 3.590754508972168\n",
      "442/1070 loss is: 3.6257946491241455\n",
      "443/1070 loss is: 4.569527626037598\n",
      "444/1070 loss is: 3.0545268058776855\n",
      "445/1070 loss is: 6.155428886413574\n",
      "446/1070 loss is: 2.878030776977539\n",
      "447/1070 loss is: 4.572510719299316\n",
      "448/1070 loss is: 4.817843914031982\n",
      "449/1070 loss is: 5.648470401763916\n",
      "450/1070 loss is: 7.190887451171875\n",
      "451/1070 loss is: 3.5973098278045654\n",
      "452/1070 loss is: 2.397538423538208\n",
      "453/1070 loss is: 5.643908500671387\n",
      "454/1070 loss is: 3.4292001724243164\n",
      "455/1070 loss is: 3.351675033569336\n",
      "456/1070 loss is: 6.461132049560547\n",
      "457/1070 loss is: 2.7944109439849854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458/1070 loss is: 4.9861955642700195\n",
      "459/1070 loss is: 4.270880699157715\n",
      "460/1070 loss is: 8.099242210388184\n",
      "461/1070 loss is: 7.025805950164795\n",
      "462/1070 loss is: 5.210071563720703\n",
      "463/1070 loss is: 4.880614280700684\n",
      "464/1070 loss is: 2.942962169647217\n",
      "465/1070 loss is: 4.247701644897461\n",
      "466/1070 loss is: 4.1097307205200195\n",
      "467/1070 loss is: 4.594721794128418\n",
      "468/1070 loss is: 2.8661653995513916\n",
      "469/1070 loss is: 5.009261131286621\n",
      "470/1070 loss is: 4.949841022491455\n",
      "471/1070 loss is: 5.95875358581543\n",
      "472/1070 loss is: 4.1394782066345215\n",
      "473/1070 loss is: 3.9243831634521484\n",
      "474/1070 loss is: 4.2740373611450195\n",
      "475/1070 loss is: 5.378903865814209\n",
      "476/1070 loss is: 4.617550373077393\n",
      "477/1070 loss is: 3.850227117538452\n",
      "478/1070 loss is: 4.662897109985352\n",
      "479/1070 loss is: 4.844101905822754\n",
      "480/1070 loss is: 6.15587043762207\n",
      "481/1070 loss is: 3.481034755706787\n",
      "482/1070 loss is: 6.749127388000488\n",
      "483/1070 loss is: 4.43846321105957\n",
      "484/1070 loss is: 2.9697046279907227\n",
      "485/1070 loss is: 5.11068058013916\n",
      "486/1070 loss is: 6.859382629394531\n",
      "487/1070 loss is: 3.484341621398926\n",
      "488/1070 loss is: 4.398419380187988\n",
      "489/1070 loss is: 3.778883457183838\n",
      "490/1070 loss is: 4.384965896606445\n",
      "491/1070 loss is: 3.7530674934387207\n",
      "492/1070 loss is: 3.4360125064849854\n",
      "493/1070 loss is: 3.260683059692383\n",
      "494/1070 loss is: 4.637657165527344\n",
      "495/1070 loss is: 2.520799160003662\n",
      "496/1070 loss is: 3.4233555793762207\n",
      "497/1070 loss is: 2.9340927600860596\n",
      "498/1070 loss is: 2.7027833461761475\n",
      "499/1070 loss is: 4.500324726104736\n",
      "500/1070 loss is: 3.566781520843506\n",
      "501/1070 loss is: 4.315919399261475\n",
      "502/1070 loss is: 3.543804168701172\n",
      "503/1070 loss is: 3.523799419403076\n",
      "504/1070 loss is: 7.771453380584717\n",
      "505/1070 loss is: 4.870424270629883\n",
      "506/1070 loss is: 2.6482577323913574\n",
      "507/1070 loss is: 3.2658863067626953\n",
      "508/1070 loss is: 5.4925737380981445\n",
      "509/1070 loss is: 3.3835482597351074\n",
      "510/1070 loss is: 5.911378860473633\n",
      "511/1070 loss is: 5.032334327697754\n",
      "512/1070 loss is: 6.051311492919922\n",
      "513/1070 loss is: 4.820993423461914\n",
      "514/1070 loss is: 4.634664535522461\n",
      "515/1070 loss is: 4.685129165649414\n",
      "516/1070 loss is: 4.232156753540039\n",
      "517/1070 loss is: 4.581037521362305\n",
      "518/1070 loss is: 4.624521255493164\n",
      "519/1070 loss is: 4.066830158233643\n",
      "520/1070 loss is: 6.05659818649292\n",
      "521/1070 loss is: 5.008767127990723\n",
      "522/1070 loss is: 3.8252670764923096\n",
      "523/1070 loss is: 3.3410000801086426\n",
      "524/1070 loss is: 2.6619834899902344\n",
      "525/1070 loss is: 3.452942371368408\n",
      "526/1070 loss is: 6.292849540710449\n",
      "527/1070 loss is: 3.1676888465881348\n",
      "528/1070 loss is: 4.471808433532715\n",
      "529/1070 loss is: 2.385403633117676\n",
      "530/1070 loss is: 3.4700963497161865\n",
      "531/1070 loss is: 4.239795684814453\n",
      "532/1070 loss is: 4.140361309051514\n",
      "533/1070 loss is: 4.0143513679504395\n",
      "534/1070 loss is: 4.694636821746826\n",
      "535/1070 loss is: 6.473100662231445\n",
      "536/1070 loss is: 2.546924591064453\n",
      "537/1070 loss is: 5.6320695877075195\n",
      "538/1070 loss is: 4.4972686767578125\n",
      "539/1070 loss is: 3.927996873855591\n",
      "540/1070 loss is: 5.969592094421387\n",
      "541/1070 loss is: 5.775052070617676\n",
      "542/1070 loss is: 4.146733283996582\n",
      "543/1070 loss is: 3.728997230529785\n",
      "544/1070 loss is: 4.036009311676025\n",
      "545/1070 loss is: 6.281329154968262\n",
      "546/1070 loss is: 4.864314556121826\n",
      "547/1070 loss is: 6.188418388366699\n",
      "548/1070 loss is: 4.808638572692871\n",
      "549/1070 loss is: 6.452136039733887\n",
      "550/1070 loss is: 3.2067012786865234\n",
      "551/1070 loss is: 6.52524471282959\n",
      "552/1070 loss is: 5.147555351257324\n",
      "553/1070 loss is: 3.973315715789795\n",
      "554/1070 loss is: 2.954704761505127\n",
      "555/1070 loss is: 4.985233783721924\n",
      "556/1070 loss is: 5.374406814575195\n",
      "557/1070 loss is: 5.1823577880859375\n",
      "558/1070 loss is: 4.751256942749023\n",
      "559/1070 loss is: 6.48382568359375\n",
      "560/1070 loss is: 3.4263787269592285\n",
      "561/1070 loss is: 3.025270462036133\n",
      "562/1070 loss is: 5.9929704666137695\n",
      "563/1070 loss is: 5.265888214111328\n",
      "564/1070 loss is: 2.3512685298919678\n",
      "565/1070 loss is: 3.5097296237945557\n",
      "566/1070 loss is: 6.710790157318115\n",
      "567/1070 loss is: 3.589989185333252\n",
      "568/1070 loss is: 3.190488815307617\n",
      "569/1070 loss is: 3.941105842590332\n",
      "570/1070 loss is: 3.7599093914031982\n",
      "571/1070 loss is: 4.061931133270264\n",
      "572/1070 loss is: 7.419563293457031\n",
      "573/1070 loss is: 9.661298751831055\n",
      "574/1070 loss is: 2.862696886062622\n",
      "575/1070 loss is: 3.259625196456909\n",
      "576/1070 loss is: 2.4102606773376465\n",
      "577/1070 loss is: 2.4561774730682373\n",
      "578/1070 loss is: 3.1740970611572266\n",
      "579/1070 loss is: 3.3115904331207275\n",
      "580/1070 loss is: 3.2326087951660156\n",
      "581/1070 loss is: 2.764603853225708\n",
      "582/1070 loss is: 4.511756896972656\n",
      "583/1070 loss is: 3.9364867210388184\n",
      "584/1070 loss is: 3.751163959503174\n",
      "585/1070 loss is: 4.0655412673950195\n",
      "586/1070 loss is: 5.7813496589660645\n",
      "587/1070 loss is: 2.7752299308776855\n",
      "588/1070 loss is: 4.8261237144470215\n",
      "589/1070 loss is: 4.527375221252441\n",
      "590/1070 loss is: 5.082943916320801\n",
      "591/1070 loss is: 4.082367420196533\n",
      "592/1070 loss is: 4.021108627319336\n",
      "593/1070 loss is: 4.698643684387207\n",
      "594/1070 loss is: 4.223944664001465\n",
      "595/1070 loss is: 4.572085857391357\n",
      "596/1070 loss is: 3.8934366703033447\n",
      "597/1070 loss is: 4.524548530578613\n",
      "598/1070 loss is: 2.3768181800842285\n",
      "599/1070 loss is: 4.012340545654297\n",
      "600/1070 loss is: 4.1327009201049805\n",
      "601/1070 loss is: 5.228812217712402\n",
      "602/1070 loss is: 6.48129940032959\n",
      "603/1070 loss is: 5.261300086975098\n",
      "604/1070 loss is: 2.6399145126342773\n",
      "605/1070 loss is: 4.843303680419922\n",
      "606/1070 loss is: 4.555642127990723\n",
      "607/1070 loss is: 2.9596025943756104\n",
      "608/1070 loss is: 2.9343385696411133\n",
      "609/1070 loss is: 6.609966278076172\n",
      "610/1070 loss is: 3.156240940093994\n",
      "611/1070 loss is: 5.971642017364502\n",
      "612/1070 loss is: 3.073530673980713\n",
      "613/1070 loss is: 3.922327995300293\n",
      "614/1070 loss is: 4.280485153198242\n",
      "615/1070 loss is: 3.4776463508605957\n",
      "616/1070 loss is: 2.5504355430603027\n",
      "617/1070 loss is: 3.0821139812469482\n",
      "618/1070 loss is: 4.090000629425049\n",
      "619/1070 loss is: 3.384033203125\n",
      "620/1070 loss is: 4.792492389678955\n",
      "621/1070 loss is: 4.591124057769775\n",
      "622/1070 loss is: 5.046695232391357\n",
      "623/1070 loss is: 2.228135824203491\n",
      "624/1070 loss is: 4.967221736907959\n",
      "625/1070 loss is: 2.5139431953430176\n",
      "626/1070 loss is: 3.720874309539795\n",
      "627/1070 loss is: 3.7863147258758545\n",
      "628/1070 loss is: 2.30039644241333\n",
      "629/1070 loss is: 4.4379167556762695\n",
      "630/1070 loss is: 3.347841739654541\n",
      "631/1070 loss is: 3.300243377685547\n",
      "632/1070 loss is: 3.577488899230957\n",
      "633/1070 loss is: 4.4375457763671875\n",
      "634/1070 loss is: 3.9262142181396484\n",
      "635/1070 loss is: 2.454622268676758\n",
      "636/1070 loss is: 5.631899833679199\n",
      "637/1070 loss is: 5.962335109710693\n",
      "638/1070 loss is: 6.412779808044434\n",
      "639/1070 loss is: 4.443942070007324\n",
      "640/1070 loss is: 3.983647346496582\n",
      "641/1070 loss is: 3.454547643661499\n",
      "642/1070 loss is: 5.641256332397461\n",
      "643/1070 loss is: 4.153097152709961\n",
      "644/1070 loss is: 5.160285949707031\n",
      "645/1070 loss is: 2.5660386085510254\n",
      "646/1070 loss is: 5.761430740356445\n",
      "647/1070 loss is: 3.1780636310577393\n",
      "648/1070 loss is: 2.4953932762145996\n",
      "649/1070 loss is: 5.200228691101074\n",
      "650/1070 loss is: 4.382152080535889\n",
      "651/1070 loss is: 4.821013450622559\n",
      "652/1070 loss is: 5.109813690185547\n",
      "653/1070 loss is: 4.18245267868042\n",
      "654/1070 loss is: 4.761713981628418\n",
      "655/1070 loss is: 2.7286529541015625\n",
      "656/1070 loss is: 4.428888320922852\n",
      "657/1070 loss is: 4.503939628601074\n",
      "658/1070 loss is: 2.5819473266601562\n",
      "659/1070 loss is: 4.272631645202637\n",
      "660/1070 loss is: 3.169281005859375\n",
      "661/1070 loss is: 3.1268370151519775\n",
      "662/1070 loss is: 3.838066816329956\n",
      "663/1070 loss is: 3.170797109603882\n",
      "664/1070 loss is: 2.6142826080322266\n",
      "665/1070 loss is: 3.584038734436035\n",
      "666/1070 loss is: 3.2903919219970703\n",
      "667/1070 loss is: 3.2294974327087402\n",
      "668/1070 loss is: 5.031384468078613\n",
      "669/1070 loss is: 6.910212516784668\n",
      "670/1070 loss is: 3.126458168029785\n",
      "671/1070 loss is: 7.2539262771606445\n",
      "672/1070 loss is: 5.23558235168457\n",
      "673/1070 loss is: 5.052130699157715\n",
      "674/1070 loss is: 3.692289352416992\n",
      "675/1070 loss is: 3.7791342735290527\n",
      "676/1070 loss is: 3.203732967376709\n",
      "677/1070 loss is: 5.202105522155762\n",
      "678/1070 loss is: 3.5395097732543945\n",
      "679/1070 loss is: 3.869546413421631\n",
      "680/1070 loss is: 3.778979539871216\n",
      "681/1070 loss is: 4.960544586181641\n",
      "682/1070 loss is: 3.993208169937134\n",
      "683/1070 loss is: 2.6245980262756348\n",
      "684/1070 loss is: 4.219407081604004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/1070 loss is: 3.5975165367126465\n",
      "686/1070 loss is: 3.520113468170166\n",
      "687/1070 loss is: 2.7522225379943848\n",
      "688/1070 loss is: 3.616110324859619\n",
      "689/1070 loss is: 4.054043769836426\n",
      "690/1070 loss is: 5.4775190353393555\n",
      "691/1070 loss is: 3.3317275047302246\n",
      "692/1070 loss is: 4.706493377685547\n",
      "693/1070 loss is: 3.648533344268799\n",
      "694/1070 loss is: 5.150812149047852\n",
      "695/1070 loss is: 3.3735191822052\n",
      "696/1070 loss is: 4.390236854553223\n",
      "697/1070 loss is: 4.034657955169678\n",
      "698/1070 loss is: 3.1693837642669678\n",
      "699/1070 loss is: 4.818690776824951\n",
      "700/1070 loss is: 3.847196340560913\n",
      "701/1070 loss is: 3.783782482147217\n",
      "702/1070 loss is: 3.20729923248291\n",
      "703/1070 loss is: 3.558377981185913\n",
      "704/1070 loss is: 7.9215192794799805\n",
      "705/1070 loss is: 3.604094982147217\n",
      "706/1070 loss is: 3.552206039428711\n",
      "707/1070 loss is: 3.917632579803467\n",
      "708/1070 loss is: 4.0839762687683105\n",
      "709/1070 loss is: 3.583249807357788\n",
      "710/1070 loss is: 4.894253253936768\n",
      "711/1070 loss is: 4.005527019500732\n",
      "712/1070 loss is: 6.556837558746338\n",
      "713/1070 loss is: 5.571865558624268\n",
      "714/1070 loss is: 5.053168296813965\n",
      "715/1070 loss is: 3.3831708431243896\n",
      "716/1070 loss is: 8.594311714172363\n",
      "717/1070 loss is: 6.855429649353027\n",
      "718/1070 loss is: 3.060980796813965\n",
      "719/1070 loss is: 5.060876369476318\n",
      "720/1070 loss is: 5.2807769775390625\n",
      "721/1070 loss is: 4.430514812469482\n",
      "722/1070 loss is: 3.8541431427001953\n",
      "723/1070 loss is: 4.163165092468262\n",
      "724/1070 loss is: 4.9626665115356445\n",
      "725/1070 loss is: 3.820971727371216\n",
      "726/1070 loss is: 3.334895133972168\n",
      "727/1070 loss is: 3.905386209487915\n",
      "728/1070 loss is: 6.284698486328125\n",
      "729/1070 loss is: 4.204631805419922\n",
      "730/1070 loss is: 5.85162353515625\n",
      "731/1070 loss is: 5.726061820983887\n",
      "732/1070 loss is: 3.424894332885742\n",
      "733/1070 loss is: 3.7157201766967773\n",
      "734/1070 loss is: 2.8130645751953125\n",
      "735/1070 loss is: 2.9974558353424072\n",
      "736/1070 loss is: 3.345341682434082\n",
      "737/1070 loss is: 4.894765853881836\n",
      "738/1070 loss is: 5.481945991516113\n",
      "739/1070 loss is: 5.852749347686768\n",
      "740/1070 loss is: 2.3515677452087402\n",
      "741/1070 loss is: 2.972738265991211\n",
      "742/1070 loss is: 4.860682487487793\n",
      "743/1070 loss is: 2.6953611373901367\n",
      "744/1070 loss is: 2.5379104614257812\n",
      "745/1070 loss is: 3.565032482147217\n",
      "746/1070 loss is: 3.6013200283050537\n",
      "747/1070 loss is: 3.5695462226867676\n",
      "748/1070 loss is: 3.7033329010009766\n",
      "749/1070 loss is: 5.874029159545898\n",
      "750/1070 loss is: 4.159359931945801\n",
      "751/1070 loss is: 3.793778896331787\n",
      "752/1070 loss is: 3.3500518798828125\n",
      "753/1070 loss is: 4.079111099243164\n",
      "754/1070 loss is: 3.9676947593688965\n",
      "755/1070 loss is: 4.704155445098877\n",
      "756/1070 loss is: 4.797171592712402\n",
      "757/1070 loss is: 4.291469573974609\n",
      "758/1070 loss is: 3.603426933288574\n",
      "759/1070 loss is: 4.421633720397949\n",
      "760/1070 loss is: 4.994723320007324\n",
      "761/1070 loss is: 4.382667064666748\n",
      "762/1070 loss is: 3.964077949523926\n",
      "763/1070 loss is: 3.8760924339294434\n",
      "764/1070 loss is: 4.558496475219727\n",
      "765/1070 loss is: 3.529035806655884\n",
      "766/1070 loss is: 5.800659656524658\n",
      "767/1070 loss is: 4.998378276824951\n",
      "768/1070 loss is: 4.25520133972168\n",
      "769/1070 loss is: 3.985330104827881\n",
      "770/1070 loss is: 3.8544411659240723\n",
      "771/1070 loss is: 3.74945068359375\n",
      "772/1070 loss is: 4.006599426269531\n",
      "773/1070 loss is: 5.656773567199707\n",
      "774/1070 loss is: 5.071351051330566\n",
      "775/1070 loss is: 6.007835388183594\n",
      "776/1070 loss is: 4.462128162384033\n",
      "777/1070 loss is: 3.449160099029541\n",
      "778/1070 loss is: 4.066095352172852\n",
      "779/1070 loss is: 3.9998795986175537\n",
      "780/1070 loss is: 4.341597557067871\n",
      "781/1070 loss is: 4.120617866516113\n",
      "782/1070 loss is: 3.27824068069458\n",
      "783/1070 loss is: 3.6374311447143555\n",
      "784/1070 loss is: 3.4741294384002686\n",
      "785/1070 loss is: 4.370014667510986\n",
      "786/1070 loss is: 2.9243004322052\n",
      "787/1070 loss is: 6.20048713684082\n",
      "788/1070 loss is: 3.548828125\n",
      "789/1070 loss is: 4.346612930297852\n",
      "790/1070 loss is: 4.792657852172852\n",
      "791/1070 loss is: 3.2917838096618652\n",
      "792/1070 loss is: 4.3864006996154785\n",
      "793/1070 loss is: 3.379882335662842\n",
      "794/1070 loss is: 4.237197399139404\n",
      "795/1070 loss is: 4.8928117752075195\n",
      "796/1070 loss is: 5.17097282409668\n",
      "797/1070 loss is: 3.53609561920166\n",
      "798/1070 loss is: 4.480852127075195\n",
      "799/1070 loss is: 5.392533302307129\n",
      "800/1070 loss is: 4.667590141296387\n",
      "801/1070 loss is: 4.833612442016602\n",
      "802/1070 loss is: 3.701345443725586\n",
      "803/1070 loss is: 3.5792489051818848\n",
      "804/1070 loss is: 4.336774826049805\n",
      "805/1070 loss is: 3.144421100616455\n",
      "806/1070 loss is: 4.851551532745361\n",
      "807/1070 loss is: 3.8123159408569336\n",
      "808/1070 loss is: 5.153138160705566\n",
      "809/1070 loss is: 2.9631187915802\n",
      "810/1070 loss is: 3.6613900661468506\n",
      "811/1070 loss is: 4.959685325622559\n",
      "812/1070 loss is: 5.240166664123535\n",
      "813/1070 loss is: 3.68269419670105\n",
      "814/1070 loss is: 5.066951751708984\n",
      "815/1070 loss is: 3.797609806060791\n",
      "816/1070 loss is: 2.6353211402893066\n",
      "817/1070 loss is: 3.6346960067749023\n",
      "818/1070 loss is: 4.111361980438232\n",
      "819/1070 loss is: 4.897883415222168\n",
      "820/1070 loss is: 4.683795928955078\n",
      "821/1070 loss is: 3.513469696044922\n",
      "822/1070 loss is: 3.876319646835327\n",
      "823/1070 loss is: 2.9074864387512207\n",
      "824/1070 loss is: 2.9780914783477783\n",
      "825/1070 loss is: 4.032930850982666\n",
      "826/1070 loss is: 3.137361526489258\n",
      "827/1070 loss is: 3.9134161472320557\n",
      "828/1070 loss is: 4.447999477386475\n",
      "829/1070 loss is: 3.200350046157837\n",
      "830/1070 loss is: 3.703732967376709\n",
      "831/1070 loss is: 3.704252243041992\n",
      "832/1070 loss is: 4.473025798797607\n",
      "833/1070 loss is: 2.9852335453033447\n",
      "834/1070 loss is: 4.574207305908203\n",
      "835/1070 loss is: 2.9526000022888184\n",
      "836/1070 loss is: 4.052396297454834\n",
      "837/1070 loss is: 5.09492301940918\n",
      "838/1070 loss is: 5.535197734832764\n",
      "839/1070 loss is: 4.6880950927734375\n",
      "840/1070 loss is: 2.8711740970611572\n",
      "841/1070 loss is: 6.179167747497559\n",
      "842/1070 loss is: 3.662216901779175\n",
      "843/1070 loss is: 4.088271141052246\n",
      "844/1070 loss is: 3.9767491817474365\n",
      "845/1070 loss is: 3.413930654525757\n",
      "846/1070 loss is: 2.762282371520996\n",
      "847/1070 loss is: 7.258338451385498\n",
      "848/1070 loss is: 5.460727691650391\n",
      "849/1070 loss is: 6.713005065917969\n",
      "850/1070 loss is: 2.43857479095459\n",
      "851/1070 loss is: 4.8962321281433105\n",
      "852/1070 loss is: 2.6455373764038086\n",
      "853/1070 loss is: 3.550086498260498\n",
      "854/1070 loss is: 4.270670413970947\n",
      "855/1070 loss is: 6.759369850158691\n",
      "856/1070 loss is: 3.900620460510254\n",
      "857/1070 loss is: 4.557211875915527\n",
      "858/1070 loss is: 3.4494311809539795\n",
      "859/1070 loss is: 4.230742931365967\n",
      "860/1070 loss is: 4.132089138031006\n",
      "861/1070 loss is: 2.963592052459717\n",
      "862/1070 loss is: 3.928884267807007\n",
      "863/1070 loss is: 7.267909526824951\n",
      "864/1070 loss is: 4.561049938201904\n",
      "865/1070 loss is: 4.026360511779785\n",
      "866/1070 loss is: 3.5792746543884277\n",
      "867/1070 loss is: 4.637513160705566\n",
      "868/1070 loss is: 4.573063850402832\n",
      "869/1070 loss is: 4.350187301635742\n",
      "870/1070 loss is: 4.048672676086426\n",
      "871/1070 loss is: 2.454659938812256\n",
      "872/1070 loss is: 5.224521636962891\n",
      "873/1070 loss is: 4.3248443603515625\n",
      "874/1070 loss is: 3.8553009033203125\n",
      "875/1070 loss is: 4.68839168548584\n",
      "876/1070 loss is: 3.875308036804199\n",
      "877/1070 loss is: 3.285702705383301\n",
      "878/1070 loss is: 3.086491823196411\n",
      "879/1070 loss is: 3.277344226837158\n",
      "880/1070 loss is: 3.707282066345215\n",
      "881/1070 loss is: 3.444077253341675\n",
      "882/1070 loss is: 3.6835145950317383\n",
      "883/1070 loss is: 6.992456436157227\n",
      "884/1070 loss is: 3.0047709941864014\n",
      "885/1070 loss is: 2.8184964656829834\n",
      "886/1070 loss is: 4.083184242248535\n",
      "887/1070 loss is: 3.3779244422912598\n",
      "888/1070 loss is: 4.399775505065918\n",
      "889/1070 loss is: 3.36716628074646\n",
      "890/1070 loss is: 5.007352352142334\n",
      "891/1070 loss is: 4.409319877624512\n",
      "892/1070 loss is: 6.321270942687988\n",
      "893/1070 loss is: 3.910308361053467\n",
      "894/1070 loss is: 3.971614122390747\n",
      "895/1070 loss is: 3.517544746398926\n",
      "896/1070 loss is: 3.6336050033569336\n",
      "897/1070 loss is: 5.289907932281494\n",
      "898/1070 loss is: 3.52266526222229\n",
      "899/1070 loss is: 5.892267227172852\n",
      "900/1070 loss is: 3.648125648498535\n",
      "901/1070 loss is: 5.122156143188477\n",
      "902/1070 loss is: 3.8157715797424316\n",
      "903/1070 loss is: 3.0356569290161133\n",
      "904/1070 loss is: 4.609854221343994\n",
      "905/1070 loss is: 5.304287910461426\n",
      "906/1070 loss is: 5.221997261047363\n",
      "907/1070 loss is: 3.8068647384643555\n",
      "908/1070 loss is: 6.582115173339844\n",
      "909/1070 loss is: 4.315805912017822\n",
      "910/1070 loss is: 4.199417591094971\n",
      "911/1070 loss is: 5.666553974151611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/1070 loss is: 3.025554656982422\n",
      "913/1070 loss is: 3.215397834777832\n",
      "914/1070 loss is: 2.640397548675537\n",
      "915/1070 loss is: 4.361527919769287\n",
      "916/1070 loss is: 4.516890525817871\n",
      "917/1070 loss is: 4.2911858558654785\n",
      "918/1070 loss is: 4.0379228591918945\n",
      "919/1070 loss is: 3.3834924697875977\n",
      "920/1070 loss is: 3.1550421714782715\n",
      "921/1070 loss is: 6.562574863433838\n",
      "922/1070 loss is: 3.9911677837371826\n",
      "923/1070 loss is: 5.345587730407715\n",
      "924/1070 loss is: 5.632210731506348\n",
      "925/1070 loss is: 5.180821418762207\n",
      "926/1070 loss is: 6.975529670715332\n",
      "927/1070 loss is: 5.398812294006348\n",
      "928/1070 loss is: 6.816868782043457\n",
      "929/1070 loss is: 4.56820821762085\n",
      "930/1070 loss is: 4.118063449859619\n",
      "931/1070 loss is: 4.397650241851807\n",
      "932/1070 loss is: 4.415583610534668\n",
      "933/1070 loss is: 4.423004627227783\n",
      "934/1070 loss is: 5.039300918579102\n",
      "935/1070 loss is: 3.5971360206604004\n",
      "936/1070 loss is: 3.457103729248047\n",
      "937/1070 loss is: 3.228330135345459\n",
      "938/1070 loss is: 5.843931198120117\n",
      "939/1070 loss is: 5.757113456726074\n",
      "940/1070 loss is: 4.087124824523926\n",
      "941/1070 loss is: 5.493529319763184\n",
      "942/1070 loss is: 3.1491427421569824\n",
      "943/1070 loss is: 4.161157608032227\n",
      "944/1070 loss is: 4.4261016845703125\n",
      "945/1070 loss is: 3.1623473167419434\n",
      "946/1070 loss is: 5.282358169555664\n",
      "947/1070 loss is: 4.546308994293213\n",
      "948/1070 loss is: 4.6280317306518555\n",
      "949/1070 loss is: 4.167489051818848\n",
      "950/1070 loss is: 3.565152645111084\n",
      "951/1070 loss is: 4.671582221984863\n",
      "952/1070 loss is: 3.945793390274048\n",
      "953/1070 loss is: 4.15311336517334\n",
      "954/1070 loss is: 3.8288755416870117\n",
      "955/1070 loss is: 4.829607963562012\n",
      "956/1070 loss is: 3.7838480472564697\n",
      "957/1070 loss is: 4.241763114929199\n",
      "958/1070 loss is: 4.719819068908691\n",
      "959/1070 loss is: 3.703521966934204\n",
      "960/1070 loss is: 4.099343299865723\n",
      "961/1070 loss is: 4.471499443054199\n",
      "962/1070 loss is: 4.164068222045898\n",
      "963/1070 loss is: 3.706141710281372\n",
      "964/1070 loss is: 4.492705821990967\n",
      "965/1070 loss is: 4.162675857543945\n",
      "966/1070 loss is: 7.879856109619141\n",
      "967/1070 loss is: 6.027679443359375\n",
      "968/1070 loss is: 3.8709335327148438\n",
      "969/1070 loss is: 3.8929805755615234\n",
      "970/1070 loss is: 4.571197032928467\n",
      "971/1070 loss is: 3.473322629928589\n",
      "972/1070 loss is: 3.5997962951660156\n",
      "973/1070 loss is: 3.524829864501953\n",
      "974/1070 loss is: 5.971922874450684\n",
      "975/1070 loss is: 4.379554748535156\n",
      "976/1070 loss is: 5.628157615661621\n",
      "977/1070 loss is: 3.766270637512207\n",
      "978/1070 loss is: 4.039600849151611\n",
      "979/1070 loss is: 4.5550537109375\n",
      "980/1070 loss is: 3.6190075874328613\n",
      "981/1070 loss is: 3.066521644592285\n",
      "982/1070 loss is: 3.0915284156799316\n",
      "983/1070 loss is: 5.385524749755859\n",
      "984/1070 loss is: 3.749851942062378\n",
      "985/1070 loss is: 4.18648624420166\n",
      "986/1070 loss is: 4.286233901977539\n",
      "987/1070 loss is: 3.875642776489258\n",
      "988/1070 loss is: 3.6934986114501953\n",
      "989/1070 loss is: 5.033288478851318\n",
      "990/1070 loss is: 5.6915717124938965\n",
      "991/1070 loss is: 3.1423606872558594\n",
      "992/1070 loss is: 4.2893242835998535\n",
      "993/1070 loss is: 2.815885305404663\n",
      "994/1070 loss is: 3.098780393600464\n",
      "995/1070 loss is: 6.045761585235596\n",
      "996/1070 loss is: 5.001613616943359\n",
      "997/1070 loss is: 3.5166406631469727\n",
      "998/1070 loss is: 4.268702507019043\n",
      "999/1070 loss is: 4.68921422958374\n",
      "1000/1070 loss is: 5.45292854309082\n",
      "1001/1070 loss is: 4.841920852661133\n",
      "1002/1070 loss is: 3.373147487640381\n",
      "1003/1070 loss is: 3.8382554054260254\n",
      "1004/1070 loss is: 4.6135454177856445\n",
      "1005/1070 loss is: 4.684275150299072\n",
      "1006/1070 loss is: 4.257360458374023\n",
      "1007/1070 loss is: 3.6205084323883057\n",
      "1008/1070 loss is: 4.525970458984375\n",
      "1009/1070 loss is: 3.711750030517578\n",
      "1010/1070 loss is: 3.0818166732788086\n",
      "1011/1070 loss is: 3.2543447017669678\n",
      "1012/1070 loss is: 5.0273613929748535\n",
      "1013/1070 loss is: 6.11541748046875\n",
      "1014/1070 loss is: 4.118402481079102\n",
      "1015/1070 loss is: 4.169172286987305\n",
      "1016/1070 loss is: 3.5923094749450684\n",
      "1017/1070 loss is: 4.287003993988037\n",
      "1018/1070 loss is: 3.338575839996338\n",
      "1019/1070 loss is: 5.376852035522461\n",
      "1020/1070 loss is: 3.309067964553833\n",
      "1021/1070 loss is: 6.007856369018555\n",
      "1022/1070 loss is: 2.6352789402008057\n",
      "1023/1070 loss is: 5.168460369110107\n",
      "1024/1070 loss is: 3.2061691284179688\n",
      "1025/1070 loss is: 5.03243350982666\n",
      "1026/1070 loss is: 4.402578353881836\n",
      "1027/1070 loss is: 3.5829153060913086\n",
      "1028/1070 loss is: 4.929800987243652\n",
      "1029/1070 loss is: 5.234225749969482\n",
      "1030/1070 loss is: 3.462224245071411\n",
      "1031/1070 loss is: 4.1517109870910645\n",
      "1032/1070 loss is: 3.010340690612793\n",
      "1033/1070 loss is: 4.771771430969238\n",
      "1034/1070 loss is: 5.800113201141357\n",
      "1035/1070 loss is: 4.233019828796387\n",
      "1036/1070 loss is: 5.4425554275512695\n",
      "1037/1070 loss is: 4.333353042602539\n",
      "1038/1070 loss is: 3.5091638565063477\n",
      "1039/1070 loss is: 3.4106688499450684\n",
      "1040/1070 loss is: 4.763776779174805\n",
      "1041/1070 loss is: 3.5804457664489746\n",
      "1042/1070 loss is: 3.789456844329834\n",
      "1043/1070 loss is: 2.7688698768615723\n",
      "1044/1070 loss is: 3.166221857070923\n",
      "1045/1070 loss is: 5.647770881652832\n",
      "1046/1070 loss is: 4.386279106140137\n",
      "1047/1070 loss is: 2.198185920715332\n",
      "1048/1070 loss is: 4.641436576843262\n",
      "1049/1070 loss is: 3.398991107940674\n",
      "1050/1070 loss is: 4.755974769592285\n",
      "1051/1070 loss is: 5.156602382659912\n",
      "1052/1070 loss is: 4.29893684387207\n",
      "1053/1070 loss is: 3.3200032711029053\n",
      "1054/1070 loss is: 3.039802074432373\n",
      "1055/1070 loss is: 3.7227087020874023\n",
      "1056/1070 loss is: 3.064366340637207\n",
      "1057/1070 loss is: 2.9340126514434814\n",
      "1058/1070 loss is: 3.7331953048706055\n",
      "1059/1070 loss is: 2.541706085205078\n",
      "1060/1070 loss is: 5.474063873291016\n",
      "1061/1070 loss is: 3.749671220779419\n",
      "1062/1070 loss is: 3.8726625442504883\n",
      "1063/1070 loss is: 3.6235640048980713\n",
      "1064/1070 loss is: 3.086336612701416\n",
      "1065/1070 loss is: 4.492895126342773\n",
      "1066/1070 loss is: 4.45393180847168\n",
      "1067/1070 loss is: 3.5457510948181152\n",
      "1068/1070 loss is: 3.593766927719116\n",
      "1069/1070 loss is: 3.5885422229766846\n",
      "1070/1070 loss is: 3.961616039276123\n",
      "1071/1070 loss is: 4.192348957061768\n",
      "14:58:00\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle = True, pin_memory = True, num_workers = 4)\n",
    "\n",
    "#init network \n",
    "count = 0 \n",
    "M = 20\n",
    "backbone = ResNetBackbone('resnet50')\n",
    "mtp = MTP(backbone, num_modes=M).to(device)\n",
    "\n",
    "# covernet = CoverNet(backbone, num_modes=64).to(device)\n",
    "#init loss function and optimizers\n",
    "# criterion = ConstantLatticeLoss(trajectories)\n",
    "\n",
    "criterion = MTPLoss(num_modes=M)\n",
    "optimizer = optim.Adam(mtp.parameters(), lr=3e-4)\n",
    "\n",
    "output_dir = '.'\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "losses = []\n",
    "print('starting 100 epochs')\n",
    "print('total batches:' + str(len(dataloader)))\n",
    "for i in range(1):\n",
    "    epoch_loss = 0 \n",
    "    count = 0 \n",
    "    for image_tensor, agent_vec, ground_truth in dataloader:\n",
    "        output = mtp(image_tensor.to(device), agent_vec.to(device))\n",
    "        loss = criterion(output, ground_truth.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count+=1\n",
    "        epoch_loss+=loss\n",
    "        print(\"{0}/{1} loss is: {2}\".format(count + 1, len(dataloader), loss))\n",
    "        \n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import map rasterizer for agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config from\n",
    "from nuscenes.eval.prediction.config import PredictionConfig, load_prediction_config\n",
    "config_name = 'predict_2020_icra.json'\n",
    "config = load_prediction_config(helper, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)\n",
    "mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())\n",
    "instance_token_img, sample_token_img = train_set[0].split('_')\n",
    "\n",
    "\n",
    "img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)\n",
    "\n",
    "plt.imshow(img)\n",
    "agent_state_vector = torch.Tensor([[helper.get_velocity_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_acceleration_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_heading_change_rate_for_agent(instance_token_img, sample_token_img)]])\n",
    "print(agent_state_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#initialize the rasterizer \n",
    "static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)\n",
    "mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())\n",
    "\n",
    "#instantiate mtp model \n",
    "M = 5\n",
    "backbone = ResNetBackbone('resnet50')\n",
    "mtp = MTP(backbone, num_modes=M)\n",
    "loss = MTPLoss()\n",
    "mtp_output = []\n",
    "\n",
    "# for i, token in enumerate(train_set):\n",
    "    \n",
    "#     instance_token_img, sample_token_img = token.split('_')\n",
    "#     img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)\n",
    "        \n",
    "#     velocity = helper.get_velocity_for_agent(instance_token_img, sample_token_img)\n",
    "#     acceleration = helper.get_acceleration_for_agent(instance_token_img, sample_token_img)\n",
    "#     heading = helper.get_heading_change_rate_for_agent(instance_token_img, sample_token_img)\n",
    "    \n",
    "#     #if any of the state agent vector has nan values then we simply continue to next data point\n",
    "#     if np.isnan(velocity) or np.isnan(acceleration) or np.isnan(heading):\n",
    "#         continue\n",
    "    \n",
    "#     #construct agent state vector \n",
    "#     agent_state_vec = torch.Tensor([[velocity, acceleration, heading]])\n",
    "    \n",
    "#     #change image from (N,N,3) -> (1, 3, N, N)\n",
    "#     image_tensor = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "#     #get output from network\n",
    "#     #Tensor of dimension [batch_size, number_of_modes * number_of_predictions_per_mode + number_of_modes]\n",
    "#     #B, M*24 + M\n",
    "#     output = mtp(image_tensor, agent_state_vec)\n",
    "\n",
    "#     prediction = output[:,:-M].detach().numpy()\n",
    "#     probabilites = output[:,-M:].squeeze(0).detach().numpy()\n",
    "#     prediction = prediction.reshape(M, config.seconds * 2, 2)\n",
    "\n",
    "#     serialized_pred = Prediction(instance_token_img, sample_token_img, prediction, probabilites).serialize()\n",
    "#     mtp_output.append(serialized_pred)\n",
    "    \n",
    "    #compute the metrics \n",
    "    \n",
    "    #go over https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/prediction/compute_metrics.py#L53\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump result of mtp into json file \n",
    "json.dump(mtp_output, open('mtp_preds.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mtp_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(predictions: List[Dict[str, Any]],\n",
    "                    helper: PredictHelper, config: PredictionConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes metrics from a set of predictions.\n",
    "    :param predictions: List of prediction JSON objects.\n",
    "    :param helper: Instance of PredictHelper that wraps the nuScenes val set.\n",
    "    :param config: Config file.\n",
    "    :return: Metrics. Nested dictionary where keys are metric names and value is a dictionary\n",
    "        mapping the Aggregator name to the results.\n",
    "    \"\"\"\n",
    "    n_preds = len(predictions)\n",
    "    containers = {metric.name: np.zeros((n_preds, metric.shape)) for metric in config.metrics}\n",
    "    for i, prediction_str in enumerate(predictions):\n",
    "        prediction = Prediction.deserialize(prediction_str)\n",
    "        ground_truth = helper.get_future_for_agent(prediction.instance, prediction.sample,\n",
    "                                                   config.seconds, in_agent_frame=True)\n",
    "        for metric in config.metrics:\n",
    "            containers[metric.name][i] = metric(ground_truth, prediction)\n",
    "    aggregations: Dict[str, Dict[str, List[float]]] = defaultdict(dict)\n",
    "    for metric in config.metrics:\n",
    "        for agg in metric.aggregators:\n",
    "            aggregations[metric.name][agg.name] = agg(containers[metric.name])\n",
    "    return aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.load(open('mtp_preds.json', \"r\"))\n",
    "results = compute_metrics(predictions, helper, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "print(agent_state_vector)\n",
    "print(img.shape)\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output has 50 entries.\n",
    "# The first 24 are x,y coordinates (in the agent frame) over the next 6 seconds at 2 Hz for the first mode.\n",
    "# The second 24 are the x,y coordinates for the second mode.\n",
    "# The last 2 are the logits of the mode probabilities\n",
    "output = mtp(image_tensor, agent_state_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[:,-M:]\n",
    "print(logits)\n",
    "probs = torch.softmax(logits, dim = 1)\n",
    "best_mode = torch.argmax(probs,dim=1).item()\n",
    "print(best_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_xy_local = helper.get_future_for_agent(instance_token_img, sample_token_img, seconds=6, in_agent_frame=False)\n",
    "future_xy_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_xy_local.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_xy_vec = future_xy_local.reshape(1, 24)\n",
    "future_xy_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape\n",
    "output[:,:-1].shapefu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_xy_vec - output[:,:-1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_xy_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
